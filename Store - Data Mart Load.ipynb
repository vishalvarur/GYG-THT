{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a29c7aa-636c-4776-85f7-29ae8acfbf72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This note book incrementally loads the data from the stage table to all the Dimensional and Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34abfe21-4c4a-4640-bccc-949c634a95ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries and functions.\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    current_timestamp,\n",
    "    to_timestamp,\n",
    "    concat,\n",
    "    substring,\n",
    "    max,\n",
    ")\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0f6bff-75c4-471a-bc2a-91615a15f977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the stage table\n",
    "df = spark.read.table(\"stg_store.stg_orders\")\n",
    "\n",
    "# Calculating the current maximum TransactionTime from fact_order table\n",
    "df_fact_orders_max_transaction_time = spark.read.table(\"store.fact_orders\")\n",
    "max_transaction_time = df_fact_orders_max_transaction_time.agg(\n",
    "    max(col(\"TransactionTime\")).alias(\"max_transaction_time\")\n",
    ").collect()[0][0]\n",
    "\n",
    "\n",
    "# Formatting the TransactionTime\n",
    "df_temp = df.withColumn(\n",
    "    \"TransactionTime\",\n",
    "    to_timestamp(\n",
    "        concat(\n",
    "            substring(col(\"TransactionTime\"), 5, 4),  # Extract year (e.g., \"2023\")\n",
    "            substring(col(\"TransactionTime\"), 9, 3),  # Extract month (e.g., \"Jan\")\n",
    "            substring(col(\"TransactionTime\"), -4, 4),  # Extract day (e.g., \"10\")\n",
    "            substring(col(\"TransactionTime\"), 11, 9),  # Extract time (e.g., \"12:30:00\")\n",
    "        ),\n",
    "        \"MMM dd yyyy HH:mm:ss\",\n",
    "    ),\n",
    ").filter((col(\"UserID\") != -1) & (col(\"UserID\") != -1) & (col(\"ItemCode\") != -1))\n",
    "\n",
    "if max_transaction_time is None:\n",
    "    # Load all records if no max_transaction_time is found\n",
    "    df_incremental = df_temp\n",
    "else:\n",
    "    # Incremental filter based on maximum TransactionTime\n",
    "    df_incremental = df_temp.filter(col(\"TransactionTime\") > max_transaction_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15140264-db0d-4936-ab15-ac0e005e2673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading the dim_users table with only new users\n",
    "dim_users_df = spark.table(\"store.dim_users\")\n",
    "\n",
    "new_records_dim_users_df = (\n",
    "    df_incremental.join(dim_users_df, on=\"UserId\", how=\"left_anti\")\n",
    "    .select(\"UserId\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Adding surrogate key and load date columns\n",
    "new_records_dim_users_with_skey_df = new_records_dim_users_df.withColumn(\n",
    "    \"user_skey\", monotonically_increasing_id().cast(\"int\")\n",
    ").withColumn(\"load_date\", current_timestamp())\n",
    "\n",
    "# Rearranging the columns as per table\n",
    "df_dim_users_rearranged = new_records_dim_users_with_skey_df.select(\n",
    "    \"user_skey\", \"userid\", \"load_date\"\n",
    ")\n",
    "\n",
    "# Appending the data to dim users table\n",
    "df_dim_users_rearranged.write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "    \"store.dim_users\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8882774a-7f41-4b5c-ba6d-7a865d2d6b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading the dim_items table with only new items\n",
    "dim_items_df = spark.table(\"store.dim_items\")\n",
    "\n",
    "new_records_dim_items_df = (\n",
    "    df_incremental.join(dim_items_df, on=\"ItemCode\", how=\"left_anti\")\n",
    "    .select(\"ItemCode\", \"ItemDescription\", \"CostPerItem\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Adding surrogate key and load date columns\n",
    "new_records_dim_items_with_skey_df = new_records_dim_items_df.withColumn(\n",
    "    \"item_skey\", monotonically_increasing_id().cast(\"int\")\n",
    ").withColumn(\"load_date\", current_timestamp())\n",
    "\n",
    "# Rearranging the columns as per table\n",
    "df_dim_items_rearranged = new_records_dim_items_with_skey_df.select(\n",
    "    \"item_skey\", \"ItemCode\", \"ItemDescription\", \"CostPerItem\", \"load_date\"\n",
    ")\n",
    "\n",
    "# Appending the data to dim items table\n",
    "df_dim_items_rearranged.write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "    \"store.dim_items\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ba1207-fd27-40d2-9618-c27bc85e6358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading the dim_country table with only new countries\n",
    "dim_country_df = spark.table(\"store.dim_country\")\n",
    "\n",
    "new_records_dim_country_df = (\n",
    "    df_incremental.join(dim_country_df, on=\"country\", how=\"left_anti\")\n",
    "    .select(\"country\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Adding surrogate key and load date columns\n",
    "new_records_dim_country_with_skey_df = new_records_dim_country_df.withColumn(\n",
    "    \"country_skey\", monotonically_increasing_id().cast(\"int\")\n",
    ").withColumn(\"load_date\", current_timestamp())\n",
    "\n",
    "# Rearranging the columns as per table\n",
    "df_dim_country_rearranged = new_records_dim_country_with_skey_df.select(\n",
    "    \"country_skey\", \"country\", \"load_date\"\n",
    ")\n",
    "\n",
    "# Appending the data to dim country table\n",
    "df_dim_country_rearranged.write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "    \"store.dim_country\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ffc480d-25ac-4634-93be-13bd8b7f86a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the updated dimension tables as dataframes\n",
    "\n",
    "df_dim_users = spark.read.table(\"store.dim_users\")\n",
    "df_dim_items = spark.read.table(\"store.dim_items\")\n",
    "df_dim_country = spark.read.table(\"store.dim_country\")\n",
    "\n",
    "# Lookup on the dimension tables to assign the respective keys\n",
    "df_fact_orders = (\n",
    "    df_incremental.join(\n",
    "        df_dim_users, df_incremental[\"UserId\"] == df_dim_users[\"userID\"], \"left\"\n",
    "    )\n",
    "    .join(df_dim_items, df_incremental[\"ItemCode\"] == df_dim_items[\"ItemCode\"], \"left\")\n",
    "    .join(\n",
    "        df_dim_country, df_incremental[\"Country\"] == df_dim_country[\"country\"], \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        monotonically_increasing_id().alias(\"key\").cast(\"int\"),\n",
    "        df_incremental[\"TransactionId\"],\n",
    "        df_incremental[\"TransactionTime\"],\n",
    "        df_dim_users[\"user_skey\"].alias(\"user_skey\"),\n",
    "        df_dim_items[\"item_skey\"].alias(\"item_skey\"),\n",
    "        df_incremental[\"NumberOfItemsPurchased\"],\n",
    "        df_dim_country[\"country_skey\"].alias(\"country_skey\"),\n",
    "        df_incremental[\"load_date\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Appending the data to fact_orders table\n",
    "df_fact_orders.write.format(\"delta\").mode(\"append\").saveAsTable(\"store.fact_orders \")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Store - Data Mart Load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}